{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SolemnShark871/Churn/blob/main/Churn_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "C00SSzCFYOYA"
      },
      "source": [
        "# ML Experiments for Churn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUonaHkJYOYD",
        "outputId": "8bb79f6d-af79-4824-b519-c30da2e4df5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Datasets_Churn-Update.ipynb',\n",
              " '.~lock.predictions_2022-01-19 08_30_05.545007.csv#',\n",
              " 'predictions_2022-01-19 08_30_05.545007.csv',\n",
              " 'configuration.py',\n",
              " '__pycache__',\n",
              " '.idea',\n",
              " 'Results_2022-01-18 17_12_10.040628',\n",
              " 'retired_people2022-01-18 10:30:52.715565.csv',\n",
              " 'non_retired_people2022-01-18 10:34:45.437345.csv',\n",
              " 'Results_2022-01-19 17:39:25.232474',\n",
              " 'Results_2022-01-19 20:22:09.864107',\n",
              " 'Results_2022-01-20 14:42:48.649244',\n",
              " 'Results_2022-01-20 14:53:57.580864',\n",
              " 'data_to_predict_2022-01-18 16:12:55.449522.csv',\n",
              " 'Results_2022-01-20 19:48:40.398198',\n",
              " 'Results_2022-01-20 20:16:52.366066',\n",
              " 'Data_for_prediction.ipynb',\n",
              " 'Results_2022-01-21 13:51:21.197030',\n",
              " 'Results_2022-01-21 13:51:30.639300',\n",
              " 'Results_2022-01-24 13:47:16.391509',\n",
              " 'Results_2022-01-24 14:25:46.575473',\n",
              " 'Results_2022-01-24 14:56:24.455407',\n",
              " 'Results_2022-01-24 15:54:38.628937',\n",
              " 'Results_2022-01-24 16:07:34.582229',\n",
              " 'Results_2022-01-24 16:07:46.698835',\n",
              " 'Results_2022-01-24 16:26:13.730285',\n",
              " 'Results_2022-01-24 16:28:09.815584',\n",
              " 'Results_2022-01-24 16:39:30.182287',\n",
              " 'Results_2022-01-24 16:43:20.440261',\n",
              " 'Results_2022-01-24 16:43:41.752975',\n",
              " 'Results_2022-01-24 16:48:40.531766',\n",
              " 'Churn_ML.ipynb']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV   #####Revisar esta librerÃ­a.\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pickle\n",
        "import datetime\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from keras import layers, models\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount= True)\n",
        "os.chdir('/content/drive/MyDrive/Churn/Churn_Alejo') #Thomas & Favio & Juan\n",
        "os.listdir(\"./\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Zm4GI51CYOYF"
      },
      "source": [
        "## 1. Preprocessing\n",
        "Importing the datasets for retired and non retired people\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rOy5kI6DYOYG"
      },
      "outputs": [],
      "source": [
        "df_retired = pd.read_csv(\"retired_people2022-01-18 10:30:52.715565.csv\", index_col=False)\n",
        "df_non_retired = pd.read_csv(\"non_retired_people2022-01-18 10:34:45.437345.csv\", index_col=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "bVDmG3DAYOYH"
      },
      "source": [
        "#### Preprocessing function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YBs-anUTYOYK"
      },
      "outputs": [],
      "source": [
        "def preprocessing(df_retired, df_non_retired):\n",
        "    #Analysis empty columns\n",
        "    list_retired = []\n",
        "    for column in df_retired.columns:\n",
        "        if df_retired[column].any() == False:\n",
        "            list_retired.append(column)\n",
        "\n",
        "    list_non_retired = []\n",
        "    for column in df_non_retired.columns:\n",
        "        if df_non_retired[column].any() == False:\n",
        "            list_non_retired.append(column)\n",
        "\n",
        "    common_lists = []\n",
        "    for i in list_retired:\n",
        "        if i in list_non_retired:\n",
        "            common_lists.append(i)\n",
        "\n",
        "    #Drop empty columns\n",
        "    df_retired.drop(columns = common_lists, inplace = True)\n",
        "    df_non_retired.drop(columns = common_lists, inplace = True)\n",
        "\n",
        "    #Delete weird data from EDAD and TIEMPO_EMP columns\n",
        "    df_retired = df_retired.where((df_retired['EDAD'] >= 16) & (df_retired['TIEMPO_EMP'] >= 0)).dropna()\n",
        "    df_non_retired = df_non_retired.where((df_non_retired['EDAD'] >= 16) & (df_non_retired['TIEMPO_EMP'] >= 0)).dropna()\n",
        "\n",
        "    #label each set\n",
        "    df_retired['LABEL'] = 1\n",
        "    df_non_retired['LABEL'] = 0\n",
        "\n",
        "    # Merging Data\n",
        "    df_merged = pd.concat([df_retired, df_non_retired], join = 'inner', ignore_index = True)\n",
        "    df_merged.drop(['PERSONA'],axis = 1, inplace=True)\n",
        "\n",
        "    return df_merged, common_lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "O_E5ElmIYOYL"
      },
      "outputs": [],
      "source": [
        "def normalize_data(df_merged):\n",
        "\n",
        "    labels = df_merged['LABEL']\n",
        "    df_merged_scaled = df_merged.loc[:, df_merged.columns != 'LABEL']\n",
        "\n",
        "    cols_ = df_merged_scaled.columns\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(df_merged_scaled)\n",
        "    df_merged_scaled = scaler.transform(df_merged_scaled)\n",
        "\n",
        "    df_merged_scaled = pd.DataFrame(df_merged_scaled, columns=cols_)\n",
        "    df_merged_scaled['LABEL'] = labels\n",
        "\n",
        "    return df_merged_scaled, scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HlS53VIBYOYM"
      },
      "outputs": [],
      "source": [
        "def pca(df, components):\n",
        "\n",
        "    labels = df['LABEL']\n",
        "    df_1 = df.loc[:, df.columns != 'LABEL']\n",
        "\n",
        "    pca_ = PCA(n_components = components)\n",
        "    pca_.fit(df_1)\n",
        "    df_1_transformed = pca_.transform(df_1)\n",
        "\n",
        "    df_pca = pd.DataFrame(df_1_transformed)\n",
        "    df_pca['LABEL'] = labels\n",
        "\n",
        "    return df_pca, pca_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "5k8RnZvOYOYM"
      },
      "source": [
        "## 2. Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "QWcufSf8YOYN"
      },
      "source": [
        "#### SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2jY_e7hVYOYN"
      },
      "outputs": [],
      "source": [
        "def svm_churn(df, param_grid):\n",
        "\n",
        "    ### Train test split FOR NUMERICAL ALGORITHMS: 20% test\n",
        "    X = df.drop(['LABEL'],axis = 1)\n",
        "    y = df['LABEL']\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify = y)\n",
        "\n",
        "    svm = SVC()\n",
        "\n",
        "    grid_search = GridSearchCV(svm, param_grid=param_grid,cv=5,verbose=2,scoring='accuracy', n_jobs=-1)\n",
        "    grid_search.fit(X_train,y_train)\n",
        "\n",
        "    best_model_params = grid_search.best_params_\n",
        "\n",
        "    best_model =   SVC(kernel = best_model_params['kernel'], C = best_model_params['C'],\n",
        "                    class_weight = best_model_params['class_weight'], gamma = best_model_params['gamma'])\n",
        "\n",
        "    best_model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = best_model.predict(X_test)\n",
        "\n",
        "    print('----------Model report on all classes ----------')\n",
        "    print(classification_report(y_test,y_pred, output_dict=True))\n",
        "\n",
        "    return best_model, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gv8OXwv51QOw"
      },
      "outputs": [],
      "source": [
        "def svm_churn_tts(df, param_grid, test_train_split):\n",
        "\n",
        "    ### Train test split FOR NUMERICAL ALGORITHMS: 20% test\n",
        "    X = df.drop(['LABEL'],axis = 1)\n",
        "    y = df['LABEL']\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_train_split, stratify = y)\n",
        "\n",
        "    svm = SVC()\n",
        "\n",
        "    grid_search = GridSearchCV(svm, param_grid=param_grid,cv=5,verbose=2,scoring='accuracy', n_jobs=-1)\n",
        "    grid_search.fit(X_train,y_train)\n",
        "\n",
        "    best_model_params = grid_search.best_params_\n",
        "\n",
        "    best_model =   SVC(kernel = best_model_params['kernel'], C = best_model_params['C'],\n",
        "                    class_weight = best_model_params['class_weight'], gamma = best_model_params['gamma'])\n",
        "\n",
        "    best_model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = best_model.predict(X_test)\n",
        "\n",
        "    print('----------Model report on all classes ----------')\n",
        "    print(classification_report(y_test,y_pred, output_dict=False))\n",
        "\n",
        "    return best_model, classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "IBbJyswCYOYN"
      },
      "source": [
        "#### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Cp21wzcPYOYO"
      },
      "outputs": [],
      "source": [
        "def log_reg_churn(df, grid_param):\n",
        "\n",
        "    ### Train test split FOR NUMERICAL ALGORITHMS: 20% test\n",
        "    X = df.drop(['LABEL'],axis = 1)\n",
        "    y = df['LABEL']\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify = y)\n",
        "\n",
        "    log_reg = LogisticRegression()\n",
        "\n",
        "    grid_search = GridSearchCV(estimator = log_reg, param_grid = grid_param, n_jobs = -1, cv = 5,\n",
        "                               verbose = 2, return_train_score = True, scoring = \"accuracy\")\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    best_model_params = grid_search.best_params_\n",
        "    print(grid_search.best_params_)\n",
        "\n",
        "    best_model = LogisticRegression(C = best_model_params['C'], penalty = best_model_params['penalty'], solver=best_model_params['solver'],\n",
        "                                    max_iter=best_model_params['max_iter'], n_jobs = -1)\n",
        "\n",
        "\n",
        "    best_model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = best_model.predict(X_test)\n",
        "\n",
        "\n",
        "    print('----------Model report on all classes ----------')\n",
        "    print(classification_report(y_test,y_pred, output_dict=True))\n",
        "\n",
        "    return best_model, classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2wakzvdBHma"
      },
      "source": [
        "#### Neural networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Sp2IDH7TBIGd"
      },
      "outputs": [],
      "source": [
        "def nn_churn(df, grid_param):\n",
        "    X = df.drop(['LABEL'], axis = 1)\n",
        "    y = df['LABEL']\n",
        "\n",
        "    # Nested function to create model, required for KerasClassifier\n",
        "    def create_model(\n",
        "                    # Default values\n",
        "                    activation: 'relu',\n",
        "                    dropout_rate : 0,\n",
        "                    init_mode: 'uniform',\n",
        "                    #weight_constraint: 1,\n",
        "                    optimizer: 'adam',\n",
        "                    hiden_layers: 2,\n",
        "                    units: [2, 2]) -> tf.keras.Sequential:\n",
        "    \n",
        "        # Create the model\n",
        "        model = Sequential()\n",
        "        model.add(Dense(X.shape[1], kernel_initializer =  init_mode, activation = activation))\n",
        "    \n",
        "        for i in range(hiden_layers):\n",
        "            model.add(Dense(units = units[i], activation = activation))\n",
        "        \n",
        "        model.add(Dropout(dropout_rate))\n",
        "        model.add(Dense(1, kernel_initializer = init_mode, activation = 'sigmoid'))\n",
        "        model.compile(loss = \"binary_crossentropy\", optimizer = optimizer, metrics = ['accuracy'])\n",
        "    \n",
        "        return model\n",
        "\n",
        "    #Model creation\n",
        "    model_nn = KerasClassifier(build_fn = create_model)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y)\n",
        "    \n",
        "    grid_search = GridSearchCV(estimator = model_nn, param_grid = grid_param, n_jobs = -1, \n",
        "                               cv = 5, verbose = 2, return_train_score = True, scoring = 'accuracy')\n",
        "    \n",
        "    grid_search.fit(X_train, y_train)\n",
        "    \n",
        "    best_model_params = grid_search.best_params_\n",
        "    print(best_model_params)\n",
        "\n",
        "    best_model = create_model(activation = best_model_params['activation'], dropout_rate = best_model_params['dropout_rate'],\n",
        "                              init_mode = best_model_params['init_mode'], optimizer = best_model_params['optimizer'], \n",
        "                             hiden_layers = best_model_params['hiden_layers'], units = best_model_params['units'])\n",
        "    \n",
        "\n",
        "    best_model.fit(X_train, y_train, batch_size = best_model_params['batch_size'], epochs = best_model_params['epochs'], verbose = 2)\n",
        "\n",
        "    y_pred = np.round(best_model.predict(X_test))\n",
        "\n",
        "\n",
        "    print('----------Model report on all classes ----------')\n",
        "    print(classification_report(y_test, y_pred, output_dict=True))\n",
        "\n",
        "    return best_model, classification_report\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "EVf89HkzYOYO"
      },
      "source": [
        "## 3. ML Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAsxRr5jYOYP",
        "outputId": "64601618-ce3a-4124-ef6c-d23b69573998"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 336 candidates, totalling 1680 fits\n",
            "----------Model report on all classes ----------\n",
            "{'0': {'precision': 0.775, 'recall': 0.7209302325581395, 'f1-score': 0.7469879518072289, 'support': 86}, '1': {'precision': 0.8811881188118812, 'recall': 0.9081632653061225, 'f1-score': 0.8944723618090452, 'support': 196}, 'accuracy': 0.851063829787234, 'macro avg': {'precision': 0.8280940594059406, 'recall': 0.814546748932131, 'f1-score': 0.8207301568081371, 'support': 282}, 'weighted avg': {'precision': 0.8488045081103855, 'recall': 0.851063829787234, 'f1-score': 0.8494948467021083, 'support': 282}}\n",
            "Fitting 5 folds for each of 112 candidates, totalling 560 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "245 fits failed out of a total of 560.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "35 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 449, in _check_solver\n",
            "    % (solver, penalty)\n",
            "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "35 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 449, in _check_solver\n",
            "    % (solver, penalty)\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "35 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 449, in _check_solver\n",
            "    % (solver, penalty)\n",
            "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "35 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 449, in _check_solver\n",
            "    % (solver, penalty)\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "35 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 459, in _check_solver\n",
            "    solver\n",
            "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "35 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1473, in fit\n",
            "    % self.l1_ratio\n",
            "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "35 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 464, in _check_solver\n",
            "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
            "ValueError: penalty='none' is not supported for the liblinear solver\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.305235   0.694765   0.69742773 0.69742773\n",
            " 0.8394179  0.69831268        nan        nan        nan        nan\n",
            " 0.7373294  0.76663127        nan 0.85446214        nan        nan\n",
            " 0.305235   0.694765   0.77464307 0.77464307 0.85094199 0.77642085\n",
            "        nan        nan        nan        nan 0.7373294  0.76663127\n",
            "        nan 0.85446214        nan        nan 0.78086136 0.75688102\n",
            " 0.85894199 0.85894199 0.8606883  0.85982694        nan        nan\n",
            "        nan        nan 0.7373294  0.76663127        nan 0.85446214\n",
            "        nan        nan 0.84473156 0.84117994 0.8633471  0.8633471\n",
            " 0.85446608 0.86156539        nan        nan        nan        nan\n",
            " 0.7373294  0.76663127        nan 0.85446214        nan        nan\n",
            " 0.85712488 0.86423992 0.84207276 0.84207276 0.83939036 0.8553471\n",
            "        nan        nan        nan        nan 0.7373294  0.76663127\n",
            "        nan 0.85446214        nan        nan 0.82345723 0.85446214\n",
            " 0.82079056 0.82079056 0.81900885 0.85446214        nan        nan\n",
            "        nan        nan 0.7373294  0.76663127        nan 0.85446214\n",
            "        nan        nan 0.79592527 0.85446214 0.79859587 0.79859587\n",
            " 0.7959292  0.85446214        nan        nan        nan        nan\n",
            " 0.7373294  0.76663127        nan 0.85357325]\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the train scores are non-finite: [       nan        nan 0.30523513 0.69476487 0.69875748 0.69875748\n",
            " 0.86512818 0.70119773        nan        nan        nan        nan\n",
            " 0.99866815 0.99179207        nan 0.94188103        nan        nan\n",
            " 0.30523513 0.69476487 0.79614397 0.79614397 0.88753368 0.79924942\n",
            "        nan        nan        nan        nan 0.99866815 0.99179207\n",
            "        nan 0.94165906        nan        nan 0.78194517 0.75887521\n",
            " 0.90905153 0.90905153 0.91015969 0.90927351        nan        nan\n",
            "        nan        nan 0.99866815 0.99179207        nan 0.94188103\n",
            "        nan        nan 0.89463223 0.89463124 0.93522644 0.93522644\n",
            " 0.93456076 0.93300915        nan        nan        nan        nan\n",
            " 0.99866815 0.99179207        nan 0.94188103        nan        nan\n",
            " 0.95075489 0.93655657 0.9607369  0.9607369  0.96051542 0.94188054\n",
            "        nan        nan        nan        nan 0.99866815 0.99179207\n",
            "        nan 0.94165906        nan        nan 0.97271571 0.94143708\n",
            " 0.9691668  0.9691668  0.96872359 0.94188103        nan        nan\n",
            "        nan        nan 0.99866815 0.99179207        nan 0.9416593\n",
            "        nan        nan 0.98203179 0.94188103 0.98025746 0.98025746\n",
            " 0.97981376 0.94188103        nan        nan        nan        nan\n",
            " 0.99866815 0.99179207        nan 0.94188103]\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'C': 1, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------Model report on all classes ----------\n",
            "{'0': {'precision': 0.7386363636363636, 'recall': 0.7558139534883721, 'f1-score': 0.7471264367816092, 'support': 86}, '1': {'precision': 0.8917525773195877, 'recall': 0.8826530612244898, 'f1-score': 0.8871794871794872, 'support': 196}, 'accuracy': 0.8439716312056738, 'macro avg': {'precision': 0.8151944704779757, 'recall': 0.819233507356431, 'f1-score': 0.8171529619805482, 'support': 282}, 'weighted avg': {'precision': 0.8450575617991719, 'recall': 0.8439716312056738, 'f1-score': 0.8444682732283614, 'support': 282}}\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:705: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "9/9 [==============================] - 1s 6ms/step - loss: 0.7276 - accuracy: 0.5856\n",
            "Epoch 2/15\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.6491 - accuracy: 0.6504\n",
            "Epoch 3/15\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.6361 - accuracy: 0.6442\n",
            "Epoch 4/15\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.5973 - accuracy: 0.6779\n",
            "Epoch 5/15\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.5831 - accuracy: 0.6903\n",
            "Epoch 6/15\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.5565 - accuracy: 0.6930\n",
            "Epoch 7/15\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.5457 - accuracy: 0.7045\n",
            "Epoch 8/15\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.5333 - accuracy: 0.7116\n",
            "Epoch 9/15\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.5282 - accuracy: 0.7045\n",
            "Epoch 10/15\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.5366 - accuracy: 0.7125\n",
            "Epoch 11/15\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5183 - accuracy: 0.7303\n",
            "Epoch 12/15\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5138 - accuracy: 0.7169\n",
            "Epoch 13/15\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.4982 - accuracy: 0.7258\n",
            "Epoch 14/15\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.4871 - accuracy: 0.7320\n",
            "Epoch 15/15\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.4826 - accuracy: 0.7320\n",
            "{'activation': 'softplus', 'batch_size': 128, 'dropout_rate': 0.8, 'epochs': 15, 'hiden_layers': 2, 'init_mode': 'glorot_normal', 'optimizer': 'RMSprop', 'units': [8, 4]}\n",
            "Epoch 1/15\n",
            "9/9 - 1s - loss: 0.7868 - accuracy: 0.6282 - 804ms/epoch - 89ms/step\n",
            "Epoch 2/15\n",
            "9/9 - 0s - loss: 0.6692 - accuracy: 0.6664 - 43ms/epoch - 5ms/step\n",
            "Epoch 3/15\n",
            "9/9 - 0s - loss: 0.6570 - accuracy: 0.6593 - 37ms/epoch - 4ms/step\n",
            "Epoch 4/15\n",
            "9/9 - 0s - loss: 0.6568 - accuracy: 0.6610 - 39ms/epoch - 4ms/step\n",
            "Epoch 5/15\n",
            "9/9 - 0s - loss: 0.6277 - accuracy: 0.6619 - 42ms/epoch - 5ms/step\n",
            "Epoch 6/15\n",
            "9/9 - 0s - loss: 0.6217 - accuracy: 0.6646 - 42ms/epoch - 5ms/step\n",
            "Epoch 7/15\n",
            "9/9 - 0s - loss: 0.5942 - accuracy: 0.6841 - 46ms/epoch - 5ms/step\n",
            "Epoch 8/15\n",
            "9/9 - 0s - loss: 0.5748 - accuracy: 0.7019 - 45ms/epoch - 5ms/step\n",
            "Epoch 9/15\n",
            "9/9 - 0s - loss: 0.5843 - accuracy: 0.6974 - 46ms/epoch - 5ms/step\n",
            "Epoch 10/15\n",
            "9/9 - 0s - loss: 0.5938 - accuracy: 0.6948 - 63ms/epoch - 7ms/step\n",
            "Epoch 11/15\n",
            "9/9 - 0s - loss: 0.5499 - accuracy: 0.6948 - 43ms/epoch - 5ms/step\n",
            "Epoch 12/15\n",
            "9/9 - 0s - loss: 0.5380 - accuracy: 0.6948 - 39ms/epoch - 4ms/step\n",
            "Epoch 13/15\n",
            "9/9 - 0s - loss: 0.5557 - accuracy: 0.6957 - 37ms/epoch - 4ms/step\n",
            "Epoch 14/15\n",
            "9/9 - 0s - loss: 0.5494 - accuracy: 0.6939 - 46ms/epoch - 5ms/step\n",
            "Epoch 15/15\n",
            "9/9 - 0s - loss: 0.5433 - accuracy: 0.6948 - 37ms/epoch - 4ms/step\n",
            "----------Model report on all classes ----------\n",
            "{'0': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 86}, '1': {'precision': 0.6950354609929078, 'recall': 1.0, 'f1-score': 0.8200836820083682, 'support': 196}, 'accuracy': 0.6950354609929078, 'macro avg': {'precision': 0.3475177304964539, 'recall': 0.5, 'f1-score': 0.4100418410041841, 'support': 282}, 'weighted avg': {'precision': 0.48307429203762386, 'recall': 0.6950354609929078, 'f1-score': 0.5699872399774474, 'support': 282}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: ram://0c92f825-0c06-4c0a-9db7-61279d6352d8/assets\n"
          ]
        }
      ],
      "source": [
        "## Creating the folder\n",
        "dir_str = 'Results_' + str(datetime.datetime.now())\n",
        "os.mkdir(dir_str)\n",
        "\n",
        "## Preprocessing\n",
        "preprocessed_data, common_empty_columns = preprocessing(df_retired, df_non_retired)\n",
        "## Save\n",
        "preprocessed_data.to_csv(os.path.join(dir_str, 'preprocessed_data.csv'))\n",
        "pickle.dump(common_empty_columns, open(os.path.join(dir_str,'common_empty_columns.pkl'), 'wb'))\n",
        "\n",
        "\n",
        "## Normalization\n",
        "scaled_data, scaler = normalize_data(preprocessed_data)\n",
        "## Save\n",
        "scaled_data.to_csv(os.path.join(dir_str,'scaled_data.csv'))\n",
        "pickle.dump(scaler, open(os.path.join(dir_str,'scaler.pkl'), 'wb'))\n",
        "\n",
        "\n",
        "## PCA\n",
        "pca_data, pca_ = pca(scaled_data, 250)\n",
        "## Save\n",
        "pca_data.to_csv(os.path.join(dir_str,'pca_data.csv'))\n",
        "pickle.dump(pca_, open(os.path.join(dir_str,'pca_model.pkl'), 'wb'))\n",
        "\n",
        "##---------------------------------------------------------------------------------------------------------\n",
        "## ML SVM\n",
        "grid_param_svm = {'C':  [0.0001, 0.001,0.01, 0.1, 1, 10, 100],\n",
        "                  'gamma' : ['scale', 'auto', 0.0001, 0.001, 0.01, 0.1, 10, 100],\n",
        "                  'class_weight': ['balanced', None],\n",
        "                  'kernel' : ['sigmoid','poly','rbf']}\n",
        "best_svm, report_svm = svm_churn(pca_data, grid_param_svm)\n",
        "## Save\n",
        "pickle.dump(best_svm, open(os.path.join(dir_str,'svm.sav'), 'wb'))\n",
        "pickle.dump(report_svm, open(os.path.join(dir_str,'svm_metrics.pkl'), 'wb'))\n",
        "\n",
        "\n",
        "##----------------------------------------------------------------------------------------------------------\n",
        "## ML Logistic Regression\n",
        "grid_param_lr = {\"penalty\": [\"l1\", \"l2\", \"elasticnet\", \"none\"],\n",
        "              \"C\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
        "              \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\", 'saga'],\n",
        "              \"max_iter\": [500]\n",
        "              }\n",
        "best_lr, report_lr = log_reg_churn(pca_data, grid_param_lr)\n",
        "## Save\n",
        "pickle.dump(best_lr, open(os.path.join(dir_str,'lr.sav'), 'wb'))\n",
        "pickle.dump(report_lr, open(os.path.join(dir_str,'lr_metrics.pkl'), 'wb'))\n",
        "\n",
        "##----------------------------------------------------------------------------------------------------------\n",
        "## ML Neural Network\n",
        "grid_param_nn = {\n",
        "    \"activation\": ['selu','softplus','softmax'],\n",
        "    \"init_mode\": ['he_normal', 'glorot_normal'],\n",
        "    \"dropout_rate\": [0.8],\n",
        "    \"units\": [[8, 4]],\n",
        "    \"optimizer\": ['RMSprop', 'Adam', 'SGD'],\n",
        "    \"hiden_layers\": [2],\n",
        "    \"epochs\": [15],\n",
        "    \"batch_size\":  [128]\n",
        "}\n",
        "best_nn, report_nn = nn_churn(pca_data, grid_param_nn)\n",
        "## Save\n",
        "pickle.dump(best_nn, open(os.path.join(dir_str,'nn.sav'), 'wb'))\n",
        "pickle.dump(report_nn, open(os.path.join(dir_str,'nn_metrics.pkl'), 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TTS for SVM\n",
        "\n",
        "## Creating the folder\n",
        "dir_str = 'Results_' + str(datetime.datetime.now())\n",
        "os.mkdir(dir_str)\n",
        "\n",
        "## Preprocessing\n",
        "preprocessed_data, common_empty_columns = preprocessing(df_retired, df_non_retired)\n",
        "## Save\n",
        "preprocessed_data.to_csv(os.path.join(dir_str, 'preprocessed_data.csv'))\n",
        "pickle.dump(common_empty_columns, open(os.path.join(dir_str,'common_empty_columns.pkl'), 'wb'))\n",
        "\n",
        "\n",
        "## Normalization\n",
        "scaled_data, scaler = normalize_data(preprocessed_data)\n",
        "## Save\n",
        "scaled_data.to_csv(os.path.join(dir_str,'scaled_data.csv'))\n",
        "pickle.dump(scaler, open(os.path.join(dir_str,'scaler.pkl'), 'wb'))\n",
        "\n",
        "\n",
        "## PCA\n",
        "pca_data, pca_ = pca(scaled_data, 250)\n",
        "## Save\n",
        "pca_data.to_csv(os.path.join(dir_str,'pca_data.csv'))\n",
        "pickle.dump(pca_, open(os.path.join(dir_str,'pca_model.pkl'), 'wb'))\n",
        "\n",
        "##---------------------------------------------------------------------------------------------------------\n",
        "## ML SVM\n",
        "grid_param_svm = {'C':  [0.0001, 0.001,0.01, 0.1, 1, 10, 100],\n",
        "                  'gamma' : ['scale', 'auto', 0.0001, 0.001, 0.01, 0.1, 10, 100],\n",
        "                  'class_weight': ['balanced', None],\n",
        "                  'kernel' : ['sigmoid','poly','rbf']}\n",
        "\n",
        "## Perform 3 test-train splits\n",
        "\n",
        "TTS = [0.2,0.1,0.05] #Try reversing this to see if that does anything \n",
        "\n",
        "\n",
        "#TODO try renaming these best_svmTTS (without for loop)\n",
        "print(\"20% Test\")\n",
        "best_svm20, report_svm20 = svm_churn_tts(pca_data, grid_param_svm, 0.2)\n",
        "print()\n",
        "\n",
        "print(\"10% Test\")\n",
        "best_svm10, report_svm10 = svm_churn_tts(pca_data, grid_param_svm, 0.1)\n",
        "print()\n",
        "\n",
        "print(\"5% Test\")\n",
        "best_svm5, report_svm5 = svm_churn_tts(pca_data, grid_param_svm, 0.05)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2P-29xY8oi_",
        "outputId": "9c6cc5de-a9fb-4c30-a2b4-31cb6f01427f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20% Test\n",
            "Fitting 5 folds for each of 336 candidates, totalling 1680 fits\n",
            "----------Model report on all classes ----------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.73      0.76        86\n",
            "           1       0.89      0.92      0.90       199\n",
            "\n",
            "    accuracy                           0.86       285\n",
            "   macro avg       0.84      0.83      0.83       285\n",
            "weighted avg       0.86      0.86      0.86       285\n",
            "\n",
            "\n",
            "10% Test\n",
            "Fitting 5 folds for each of 336 candidates, totalling 1680 fits\n",
            "----------Model report on all classes ----------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.74      0.80        43\n",
            "           1       0.90      0.95      0.92       100\n",
            "\n",
            "    accuracy                           0.89       143\n",
            "   macro avg       0.88      0.85      0.86       143\n",
            "weighted avg       0.89      0.89      0.89       143\n",
            "\n",
            "\n",
            "5% Test\n",
            "Fitting 5 folds for each of 336 candidates, totalling 1680 fits\n",
            "----------Model report on all classes ----------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.77      0.85        22\n",
            "           1       0.91      0.98      0.94        50\n",
            "\n",
            "    accuracy                           0.92        72\n",
            "   macro avg       0.93      0.88      0.90        72\n",
            "weighted avg       0.92      0.92      0.91        72\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUA3Qefv527s"
      },
      "outputs": [],
      "source": [
        "## Save\n",
        "pickle.dump(best_svm, open(os.path.join(dir_str,'svm.sav'), 'wb'))\n",
        "pickle.dump(report_svm, open(os.path.join(dir_str,'svm_metrics.pkl'), 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "zig7k0InYOYQ"
      },
      "source": [
        "## 4. Inference\n",
        "\n",
        "This just works after you execute the queries to predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bO_EUGu5YOYQ"
      },
      "outputs": [],
      "source": [
        "df_from_queries = pd.read_csv('data_to_predict_2022-01-18 16:12:55.449522.csv', index_col=False)\n",
        "\n",
        "def inference(df, model, common_lists, scaler, pca_model):\n",
        "\n",
        "    ids = df['PERSONA']\n",
        "    df.drop(['PERSONA'],axis = 1, inplace=True)\n",
        "\n",
        "    df.drop(columns = common_lists, inplace = True)\n",
        "    df_scaled = scaler.transform(df)\n",
        "    data_pca = pca_model.transform(df_scaled)\n",
        "\n",
        "    predictions = model.predict(data_pca)\n",
        "\n",
        "    df_predictions = pd.DataFrame(predictions)\n",
        "    df_predictions['PERSONAS'] = ids\n",
        "\n",
        "    df_predictions.to_csv('predictions_' + str(datetime.datetime.now()) + '.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InP5YSEOYOYR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "44da7756-67b2-425b-f8fb-522bfd2e36e4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-efe3077ae393>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Results_2022-01-18 17:12:10.040628'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcommon_empty_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'common_empty_columns.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'scaler.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpca_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pca_model.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Results_2022-01-18 17:12:10.040628/common_empty_columns.pkl'"
          ]
        }
      ],
      "source": [
        "## EXAMPLE\n",
        "\n",
        "folder = 'Results_2022-01-18 17:12:10.040628'\n",
        "common_empty_columns = pickle.load(open(os.path.join(folder, 'common_empty_columns.pkl'),'rb'))\n",
        "scaler = pickle.load(open(os.path.join(folder, 'scaler.pkl'),'rb'))\n",
        "pca_model = pickle.load(open(os.path.join(folder, 'pca_model.pkl'),'rb'))\n",
        "model = pickle.load(open(os.path.join(folder, 'lr.sav'),'rb'))\n",
        "\n",
        "inference(df_from_queries, model, common_empty_columns, scaler, pca_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzm1rPHkYOYR"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "zig7k0InYOYQ"
      ],
      "name": "Churn_ML.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}